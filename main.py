"""
ë‰´ìŠ¤ë ˆí„° ìš”ì•½ ì‹œìŠ¤í…œ - ë©”ì¸ ì‹¤í–‰ íŒŒì¼
"""

import sys
from typing import List, Dict
from dotenv import load_dotenv

from config import OPENAI_API_KEY, DEFAULT_ARTICLE_LIMIT, LOG_FILE
from rss_reader import RSSReader
from summarizer import ArticleSummarizer
from content_extractor import ContentExtractor
from html_exporter import HTMLExporter
from logger_config import configure_logging, get_logger, log_article_context, log_processing_step

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ë¡œê¹… ì„¤ì •
configure_logging(log_file=LOG_FILE, log_level="INFO")
logger = get_logger(__name__)

class NewsletterSummarySystem:
    """ë‰´ìŠ¤ë ˆí„° ìš”ì•½ ì‹œìŠ¤í…œ ë©”ì¸ í´ë˜ìŠ¤"""
    
    def __init__(self):
        """ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        self.api_key = OPENAI_API_KEY
        if not self.api_key:
            raise ValueError(
                "OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\n"
                ".env íŒŒì¼ì— OPENAI_API_KEY=your-api-key-hereë¥¼ ì¶”ê°€í•˜ì„¸ìš”."
            )
        
        self.rss_reader = RSSReader()
        self.summarizer = ArticleSummarizer(self.api_key)
        self.content_extractor = ContentExtractor()
        self.html_exporter = HTMLExporter()
        
        logger.info("ë‰´ìŠ¤ë ˆí„° ìš”ì•½ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ", system="newsletter_summary")
    
    def run(self, limit: int = DEFAULT_ARTICLE_LIMIT) -> List[Dict]:
        """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
        try:
            logger.info("ë‰´ìŠ¤ë ˆí„° ìš”ì•½ ì‹œì‘", **log_processing_step("start", limit=limit))
            
            # 1ë‹¨ê³„: RSS í”¼ë“œì—ì„œ ê¸°ì‚¬ ìˆ˜ì§‘
            articles = self.rss_reader.fetch_all_feeds(limit)
            
            if not articles:
                logger.warning("ìˆ˜ì§‘ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤", **log_processing_step("rss_fetch", result="no_articles"))
                return []
            
            # 2ë‹¨ê³„: ê° ê¸°ì‚¬ ìš”ì•½
            summaries = []
            for idx, article in enumerate(articles, 1):
                try:
                    article_context = log_article_context(
                        article_id=str(idx),
                        title=article['title'],
                        source=article.get('source', '')
                    )
                    logger.info("ê¸°ì‚¬ ì²˜ë¦¬ ì‹œì‘", 
                              progress=f"{idx}/{len(articles)}", 
                              **article_context)
                    
                    # ê¸°ì‚¬ ë‚´ìš© ì¤€ë¹„
                    content = article.get('content') or article.get('summary') or article.get('title', '')
                    
                    # ìš”ì•½ ìƒì„±
                    summary_result = self.summarizer.summarize_article(
                        title=article['title'],
                        content=content,
                        source=article['source']
                    )
                    
                    # ì´ë¯¸ì§€ì™€ ì°¸ê³ ë¬¸í—Œ ì¶”ì¶œ
                    article_url = article.get('link', '')
                    if article_url:
                        logger.info("ì½˜í…ì¸  ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì‹œì‘", 
                                  url=article_url, 
                                  **log_processing_step("extract_metadata"))
                        metadata = self.content_extractor.extract_content_metadata(article_url)
                        
                        # ì¶”ê°€ ì •ë³´ ì¶”ê°€
                        summary_result.update({
                            'link': article_url,
                            'published': article.get('published', ''),
                            'category': article.get('category', ''),
                            'images': metadata.get('images', []),
                            'references': metadata.get('references', []),
                            'author': metadata.get('author', ''),
                            'description': metadata.get('description', '')
                        })
                    else:
                        # URLì´ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ ì •ë³´ë§Œ ì¶”ê°€
                        summary_result.update({
                            'link': '',
                            'published': article.get('published', ''),
                            'category': article.get('category', ''),
                            'images': [],
                            'references': [],
                            'author': '',
                            'description': ''
                        })
                    
                    summaries.append(summary_result)
                    
                    # ê²°ê³¼ ì¶œë ¥
                    self._print_summary(idx, summary_result)
                    
                except Exception as e:
                    logger.error("ê¸°ì‚¬ ìš”ì•½ ì‹¤íŒ¨", 
                               error=str(e),
                               **log_article_context(str(idx), article.get('title', 'Unknown')))
                    continue
            
            logger.info("ë‰´ìŠ¤ë ˆí„° ìš”ì•½ ì™„ë£Œ", 
                       **log_processing_step("complete", processed_count=len(summaries)))
            
            # HTML íŒŒì¼ë¡œ ë‚´ë³´ë‚´ê¸°
            if summaries:
                try:
                    # output í´ë”ì— ë‚´ë³´ë‚´ê¸°
                    html_filepath = self.html_exporter.export_summaries_to_html(
                        summaries, 
                        theme="default"
                    )
                    logger.info("HTML íŒŒì¼ ìƒì„± ì™„ë£Œ", 
                              filepath=html_filepath,
                              **log_processing_step("export_html"))
                    print(f"\nğŸŒ HTML íŒŒì¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤: {html_filepath}")
                    
                    # docs í´ë”ì—ë„ ë‚´ë³´ë‚´ê¸° (GitHub Pagesìš©)
                    docs_filepath = self.html_exporter.export_to_docs(
                        summaries,
                        theme="default"
                    )
                    logger.info("GitHub Pages íŒŒì¼ ì—…ë°ì´íŠ¸ ì™„ë£Œ", 
                              filepath=docs_filepath,
                              **log_processing_step("export_docs"))
                    print(f"ğŸ“„ GitHub Pages íŒŒì¼ ì—…ë°ì´íŠ¸: {docs_filepath}")
                    print("   ë¸Œë¼ìš°ì €ì—ì„œ ì—´ì–´ì„œ ì´ë¯¸ì§€ì™€ í•¨ê»˜ í™•ì¸í•˜ì„¸ìš”!")
                except Exception as e:
                    logger.error("HTML íŒŒì¼ ìƒì„± ì‹¤íŒ¨", 
                               error=str(e),
                               **log_processing_step("export_html", status="failed"))
            
            return summaries
            
        except Exception as e:
            logger.error("ì‹œìŠ¤í…œ ì‹¤í–‰ ì‹¤íŒ¨", 
                        error=str(e),
                        **log_processing_step("system_run", status="failed"))
            raise
    
    def _print_summary(self, idx: int, summary: Dict):
        """ìš”ì•½ ê²°ê³¼ ì¶œë ¥"""
        print(f"\n{'='*60}")
        print(f"[{idx}] {summary['title']}")
        print(f"{'='*60}")
        print(f"ğŸ“° ì¶œì²˜: {summary['source']}")
        print(f"ğŸ”— ë§í¬: {summary['link']}")
        print(f"ğŸ“… ë°œí–‰ì¼: {summary.get('published', 'N/A')}")
        if summary.get('author'):
            print(f"âœï¸  ì €ì: {summary['author']}")
        print(f"ğŸ·ï¸  í‚¤ì›Œë“œ: {', '.join(summary['keywords'])}")
        
        # ì´ë¯¸ì§€ ì •ë³´ ì¶œë ¥
        images = summary.get('images', [])
        if images:
            print(f"\nğŸ–¼ï¸  ì´ë¯¸ì§€ ({len(images)}ê°œ):")
            for i, img in enumerate(images[:3], 1):  # ìµœëŒ€ 3ê°œë§Œ í‘œì‹œ
                print(f"  {i}. {img['url']}")
                if img.get('alt'):
                    print(f"     ì„¤ëª…: {img['alt']}")
            if len(images) > 3:
                print(f"     ... ì™¸ {len(images) - 3}ê°œ ë”")
        
        # ì°¸ê³ ë¬¸í—Œ ì •ë³´ ì¶œë ¥
        references = summary.get('references', [])
        if references:
            print(f"\nğŸ“š ì°¸ê³ ë¬¸í—Œ ({len(references)}ê°œ):")
            for i, ref in enumerate(references[:3], 1):  # ìµœëŒ€ 3ê°œë§Œ í‘œì‹œ
                print(f"  {i}. {ref['text']}")
                print(f"     ë§í¬: {ref['url']}")
            if len(references) > 3:
                print(f"     ... ì™¸ {len(references) - 3}ê°œ ë”")
        
        print(f"\nğŸ“ ìš”ì•½:")
        print(f"{summary['summary']}")
        print(f"\nâ° ì²˜ë¦¬ì‹œê°„: {summary['timestamp']}")
        print(f"{'='*60}")
    
    def show_feed_info(self):
        """í”¼ë“œ ì •ë³´ ì¶œë ¥"""
        feeds = self.rss_reader.get_feed_info()
        print("\nğŸ“¡ ì„¤ì •ëœ RSS í”¼ë“œ:")
        for i, feed in enumerate(feeds, 1):
            print(f"  {i}. {feed['name']} ({feed['category']})")
            print(f"     URL: {feed['url']}")
        print()

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    try:
        # ëª…ë ¹í–‰ ì¸ìˆ˜ ì²˜ë¦¬
        limit = DEFAULT_ARTICLE_LIMIT
        if len(sys.argv) > 1:
            try:
                limit = int(sys.argv[1])
            except ValueError:
                print("ì‚¬ìš©ë²•: python main.py [ê¸°ì‚¬ìˆ˜]")
                print("ì˜ˆì‹œ: python main.py 5")
                return
        
        # ì‹œìŠ¤í…œ ì‹¤í–‰
        system = NewsletterSummarySystem()
        
        # í”¼ë“œ ì •ë³´ ì¶œë ¥
        system.show_feed_info()
        
        # ìš”ì•½ ì‹¤í–‰
        summaries = system.run(limit)
        
        if summaries:
            print(f"\nâœ… ì´ {len(summaries)}ê°œ ê¸°ì‚¬ ìš”ì•½ ì™„ë£Œ!")
        else:
            print("\nâŒ ìš”ì•½í•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            
    except KeyboardInterrupt:
        print("\n\nâ¹ï¸  ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        logger.error("í”„ë¡œê·¸ë¨ ì‹¤í–‰ ì‹¤íŒ¨", error=str(e))
        print(f"\nâŒ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

if __name__ == "__main__":
    main() 